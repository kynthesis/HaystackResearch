{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvE/EhJop522hU3OvOPyiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kynthesis/HaystackResearch/blob/main/WebSearch_LFQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cách xây dựng một hệ thống QA lấy thông tin từ web search**"
      ],
      "metadata": {
        "id": "ZU0sftQVcOlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Kiểm tra GPU runtime"
      ],
      "metadata": {
        "id": "Bt9ZZWOka3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cekDh9CALGdO",
        "outputId": "ab1136c1-5d61-4b38-ca96-c7f0e7504270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug 18 05:44:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Cài đặt Haystack"
      ],
      "metadata": {
        "id": "2tex9lSma5YB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC6Mx8IMim3h"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,inference]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Nhập API của Serper và OpenAI"
      ],
      "metadata": {
        "id": "vohcmTuHa7Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "serper_api_key = getpass(\"Enter Serper API key:\")\n",
        "openai_api_key = getpass(\"Enter OpenAI API key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N97Kk_aYLyX_",
        "outputId": "2baf6260-7f08-4817-fe2f-7973ccf67e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Serper API key:··········\n",
            "Enter OpenAI API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Import các Node cần thiết"
      ],
      "metadata": {
        "id": "WECMcUw1a83y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import PromptNode, PromptTemplate, TopPSampler\n",
        "from haystack.nodes.retriever.web import WebRetriever\n",
        "from haystack.pipelines import WebQAPipeline"
      ],
      "metadata": {
        "id": "3QYGlqovNKeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Setup Prompt và Khởi tạo Pipeline"
      ],
      "metadata": {
        "id": "m-YUBntUbFcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"\"\"\n",
        "Synthesize a comprehensive answer from the following most relevant paragraphs and the given question.\n",
        "Provide a clear and concise response that summarizes the key points and information presented in the paragraphs.\n",
        "Your answer should be in your own words and be no longer than 50 words.\n",
        "\\n\\n Paragraphs: {documents} \\n\\n Question: {query} \\n\\n Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_node = PromptNode(\n",
        "    \"text-davinci-003\", default_prompt_template=PromptTemplate(prompt_text), api_key=openai_api_key, max_length=256\n",
        ")\n",
        "\n",
        "web_retriever = WebRetriever(api_key=serper_api_key, top_search_results=5, mode=\"preprocessed_documents\", top_k=30)\n",
        "\n",
        "pipeline = WebQAPipeline(retriever=web_retriever, prompt_node=prompt_node, sampler=TopPSampler(top_p=0.8))"
      ],
      "metadata": {
        "id": "si2TeYJ2NdHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Chuẩn bị câu hỏi cho hệ thống QA"
      ],
      "metadata": {
        "id": "DMMsnMsmbPtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the differences between ChatGPT-3 and ChatGPT-4?\",\n",
        "    \"When should Extractive QA and Generative QA be used?\",\n",
        "    \"What are the differences between Langchain and Haystack?\"\n",
        "]"
      ],
      "metadata": {
        "id": "sAaw6P1lNhPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Lọc các thông báo lỗi từ thư viện (optional)"
      ],
      "metadata": {
        "id": "F-TsgbGXbp18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(\"haystack.nodes.retriever.link_content\")\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "logger = logging.getLogger(\"boilerpy3\")\n",
        "logger.setLevel(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "1aTPhvyoPz-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Tiến hành thực hiện tác vụ QA và Nhận câu trả lời"
      ],
      "metadata": {
        "id": "jk_MqI00b1q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for q in questions:\n",
        "    print(f\"Question: {q}\")\n",
        "    response = pipeline.run(query=q)\n",
        "    print(f\"Answer: {response['results'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tzRW762NyeQ",
        "outputId": "dd1afb6f-df7c-478a-cb59-fcb5f174a25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the differences between ChatGPT-3 and ChatGPT-4?\n",
            "Answer: ChatGPT-4 is more powerful than ChatGPT-3, with more parameters, better understanding of context, domain-wise expertise, multi-language capabilities, and the ability to process images as well as text. It has a greater capacity to process information, a higher accuracy of responses, and fewer unsafe responses.\n",
            "Question: When should Extractive QA and Generative QA be used?\n",
            "Answer: Extractive QA should be used when the answer is present in a given context, such as a text, table, or HTML. Generative QA can be used when the answer must be generated based on a context, or when no context is provided and the answer must be completely generated by a model.\n",
            "Question: What are the differences between Langchain and Haystack?\n",
            "Answer: LangChain and Haystack both offer different approaches to building language applications. LangChain uses LLMs and Agents to delegate actions to build powerful applications, while Haystack focuses more on building large-scale search systems, question-answering, summarization and conversational AI. Haystack is currently working on adding Agent components to their platform.\n"
          ]
        }
      ]
    }
  ]
}